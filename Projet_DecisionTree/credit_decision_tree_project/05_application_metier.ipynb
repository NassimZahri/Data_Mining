{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partie 5 - Application Metier: Prediction du Defaut de Credit\n",
    "\n",
    "Ce notebook applique les modeles developpes a un cas d'usage reel dans le domaine bancaire.\n",
    "\n",
    "**Contenu:**\n",
    "- Description du domaine d'application\n",
    "- Analyse exploratoire des donnees\n",
    "- Entrainement du modele final\n",
    "- Extraction des regles de decision\n",
    "- Interpretation et discussion\n",
    "\n",
    "**Auteur**: Projet Data Mining - Arbres de Decision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Description du Domaine d'Application\n",
    "\n",
    "### 1.1 Problematique Metier\n",
    "\n",
    "Dans le secteur bancaire, l'evaluation du risque de credit est cruciale pour:\n",
    "- **Minimiser les pertes** dues aux defauts de paiement\n",
    "- **Optimiser le portefeuille** de credits accordes\n",
    "- **Respecter les reglementations** (Bale III, etc.)\n",
    "\n",
    "### 1.2 Objectif\n",
    "\n",
    "Predire si un client sera en defaut de paiement (\"oui\") ou non (\"non\") en fonction de ses caracteristiques.\n",
    "\n",
    "### 1.3 Variables\n",
    "\n",
    "- **Variables explicatives (features)**:\n",
    "  - Caracteristiques du client (statut proprietaire, situation matrimoniale, etc.)\n",
    "  - Informations financieres (revenu, etc.)\n",
    "\n",
    "- **Variable cible**:\n",
    "  - `defaut`: oui/non (classification binaire)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Importation et Exploration des Donnees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree, export_text\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "\n",
    "# Configuration\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des donnees\n",
    "base_url = 'https://raw.githubusercontent.com/NassimZahri/Data_Mining/main/data/'\n",
    "df = pd.read_csv(base_url + 'credit_simple.csv')\n",
    "\n",
    "print(\"Dimensions du dataset:\", df.shape)\n",
    "print(\"\\nPremiers enregistrements:\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informations sur le dataset\n",
    "print(\"Informations sur les colonnes:\")\n",
    "print(\"=\" * 60)\n",
    "for col in df.columns:\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(f\"  - Type: {df[col].dtype}\")\n",
    "    print(f\"  - Valeurs uniques: {df[col].nunique()}\")\n",
    "    print(f\"  - Valeurs: {df[col].unique().tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution de la variable cible\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Graphique en barres\n",
    "target_counts = df['defaut'].value_counts()\n",
    "colors = ['#27ae60', '#e74c3c']\n",
    "axes[0].bar(target_counts.index, target_counts.values, color=colors)\n",
    "axes[0].set_xlabel('Defaut de paiement')\n",
    "axes[0].set_ylabel('Nombre de clients')\n",
    "axes[0].set_title('Distribution de la Variable Cible')\n",
    "for i, v in enumerate(target_counts.values):\n",
    "    axes[0].text(i, v + 0.1, str(v), ha='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Graphique en camembert\n",
    "axes[1].pie(target_counts.values, labels=target_counts.index, autopct='%1.1f%%', \n",
    "            colors=colors, startangle=90)\n",
    "axes[1].set_title('Proportion des Defauts')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse par variable categorielle\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "categorical_cols = df.columns[:-1].tolist()  # Toutes sauf 'defaut'\n",
    "\n",
    "for idx, col in enumerate(categorical_cols[:4]):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    \n",
    "    # Crosstab\n",
    "    ct = pd.crosstab(df[col], df['defaut'], normalize='index') * 100\n",
    "    ct.plot(kind='bar', ax=ax, color=['#27ae60', '#e74c3c'])\n",
    "    \n",
    "    ax.set_xlabel(col)\n",
    "    ax.set_ylabel('Pourcentage')\n",
    "    ax.set_title(f'Taux de defaut par {col}')\n",
    "    ax.legend(title='Defaut', loc='upper right')\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preparation des Donnees et Entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparation\n",
    "X = pd.get_dummies(df.drop('defaut', axis=1))\n",
    "y = df['defaut'].map({'oui': 1, 'non': 0})\n",
    "\n",
    "# Division\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Features utilisees ({len(X.columns)}):\")\n",
    "for col in X.columns:\n",
    "    print(f\"  - {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrainement du modele final: Arbre de decision avec profondeur optimale\n",
    "# On choisit un arbre interpretable pour l'application metier\n",
    "\n",
    "# Trouver la meilleure profondeur par validation croisee\n",
    "depths = range(1, 8)\n",
    "cv_scores = []\n",
    "\n",
    "for d in depths:\n",
    "    model = DecisionTreeClassifier(max_depth=d, random_state=42)\n",
    "    scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')\n",
    "    cv_scores.append(scores.mean())\n",
    "\n",
    "best_depth = depths[np.argmax(cv_scores)]\n",
    "print(f\"Meilleure profondeur par validation croisee: {best_depth}\")\n",
    "print(f\"Score CV: {max(cv_scores):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modele final: Arbre de decision\n",
    "tree_model = DecisionTreeClassifier(max_depth=best_depth, random_state=42)\n",
    "tree_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = tree_model.predict(X_test)\n",
    "y_pred_proba = tree_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"Performance du modele sur l'ensemble de test:\")\n",
    "print(\"=\" * 60)\n",
    "print(classification_report(y_test, y_pred, target_names=['Non', 'Oui']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualisation de l'Arbre de Decision Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation graphique\n",
    "plt.figure(figsize=(20, 12))\n",
    "plot_tree(\n",
    "    tree_model,\n",
    "    feature_names=X.columns.tolist(),\n",
    "    class_names=['Non', 'Oui'],\n",
    "    filled=True,\n",
    "    rounded=True,\n",
    "    fontsize=11,\n",
    "    proportion=True\n",
    ")\n",
    "plt.title('Arbre de Decision pour la Prediction du Defaut de Credit', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Extraction des Regles de Decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regles en format texte\n",
    "print(\"Regles de decision extraites:\")\n",
    "print(\"=\" * 60)\n",
    "tree_rules = export_text(tree_model, feature_names=X.columns.tolist())\n",
    "print(tree_rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_rules_readable(tree, feature_names, class_names):\n",
    "    \"\"\"\n",
    "    Extrait les regles de l'arbre sous forme lisible pour un non-expert.\n",
    "    \"\"\"\n",
    "    from sklearn.tree import _tree\n",
    "    \n",
    "    tree_ = tree.tree_\n",
    "    feature_name = [\n",
    "        feature_names[i] if i != _tree.TREE_UNDEFINED else \"undefined!\"\n",
    "        for i in tree_.feature\n",
    "    ]\n",
    "    \n",
    "    rules = []\n",
    "    \n",
    "    def recurse(node, path):\n",
    "        if tree_.feature[node] != _tree.TREE_UNDEFINED:\n",
    "            name = feature_name[node]\n",
    "            threshold = tree_.threshold[node]\n",
    "            \n",
    "            # Branche gauche\n",
    "            recurse(tree_.children_left[node], path + [(name, \"<=\", threshold)])\n",
    "            # Branche droite\n",
    "            recurse(tree_.children_right[node], path + [(name, \">\", threshold)])\n",
    "        else:\n",
    "            # C'est une feuille\n",
    "            class_idx = np.argmax(tree_.value[node])\n",
    "            class_name = class_names[class_idx]\n",
    "            samples = tree_.n_node_samples[node]\n",
    "            rules.append((path, class_name, samples))\n",
    "    \n",
    "    recurse(0, [])\n",
    "    return rules\n",
    "\n",
    "\n",
    "rules = extract_rules_readable(tree_model, X.columns.tolist(), ['Non', 'Oui'])\n",
    "\n",
    "print(\"Regles de decision en langage naturel:\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "for i, (conditions, prediction, samples) in enumerate(rules, 1):\n",
    "    rule_text = \" ET \".join([f\"{c[0]} {c[1]} {c[2]:.2f}\" for c in conditions])\n",
    "    defaut_label = \"OUI (defaut)\" if prediction == 'Oui' else \"NON (pas de defaut)\"\n",
    "    print(f\"Regle {i}:\")\n",
    "    print(f\"  SI {rule_text}\")\n",
    "    print(f\"  ALORS Prediction = {defaut_label} ({samples} exemples)\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Importance des Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importance des features\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': tree_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "# Ne garder que les features avec importance > 0\n",
    "importance_df = importance_df[importance_df['Importance'] > 0]\n",
    "\n",
    "print(\"Importance des variables dans la decision:\")\n",
    "print(\"=\" * 60)\n",
    "importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphique d'importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "colors = plt.cm.RdYlGn_r(np.linspace(0.2, 0.8, len(importance_df)))\n",
    "plt.barh(importance_df['Feature'], importance_df['Importance'], color=colors)\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Variable')\n",
    "plt.title('Importance des Variables pour la Prediction du Defaut')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Comparaison avec Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modele Random Forest pour comparaison\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "print(\"Performance Random Forest:\")\n",
    "print(\"=\" * 60)\n",
    "print(classification_report(y_test, y_pred_rf, target_names=['Non', 'Oui']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaison des matrices de confusion\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "for idx, (name, y_p) in enumerate([('Arbre de Decision', y_pred), ('Random Forest', y_pred_rf)]):\n",
    "    cm = confusion_matrix(y_test, y_p)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx],\n",
    "                xticklabels=['Non', 'Oui'], yticklabels=['Non', 'Oui'])\n",
    "    axes[idx].set_xlabel('Prediction')\n",
    "    axes[idx].set_ylabel('Vraie valeur')\n",
    "    axes[idx].set_title(f'Matrice de Confusion - {name}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Discussion et Interpretation\n",
    "\n",
    "### 8.1 Interpretabilite\n",
    "\n",
    "**Arbre de decision:**\n",
    "- Regles claires et comprehensibles\n",
    "- Peut etre explique a un non-expert (client, regulateur)\n",
    "- Facilite l'audit et la conformite reglementaire\n",
    "\n",
    "**Random Forest:**\n",
    "- Meilleure performance predictive (potentiellement)\n",
    "- Modele \"boite noire\" - difficile a expliquer\n",
    "- Importance des features disponible mais pas les regles\n",
    "\n",
    "### 8.2 Limites Observees\n",
    "\n",
    "1. **Taille du dataset**: Avec peu de donnees, les performances sont limitees\n",
    "2. **Desequilibre des classes**: Peut biaiser les predictions\n",
    "3. **Variables manquantes**: Le modele ne dispose que de quelques variables\n",
    "4. **Donnees manquantes**: Non traitees dans cet exemple\n",
    "\n",
    "### 8.3 Recommandations Metier\n",
    "\n",
    "1. **Pour l'interpretabilite**: Utiliser l'arbre de decision simple\n",
    "2. **Pour la performance**: Utiliser Random Forest ou ensemble\n",
    "3. **Pour la production**: Combiner les deux (arbre pour explication, RF pour decision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume final\n",
    "print(\"RESUME DU PROJET\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "print(\"Domaine d'application: Prediction du defaut de credit bancaire\")\n",
    "print()\n",
    "print(\"Modele final: Arbre de decision (interpretable)\")\n",
    "print(f\"  - Profondeur: {best_depth}\")\n",
    "print(f\"  - Accuracy: {(y_pred == y_test).mean():.2%}\")\n",
    "print()\n",
    "print(\"Variables les plus importantes:\")\n",
    "for _, row in importance_df.head(3).iterrows():\n",
    "    print(f\"  - {row['Feature']}: {row['Importance']:.2%}\")\n",
    "print()\n",
    "print(\"Avantages:\")\n",
    "print(\"  - Regles explicables\")\n",
    "print(\"  - Conformite reglementaire\")\n",
    "print(\"  - Decisions auditables\")\n",
    "print()\n",
    "print(\"Limites:\")\n",
    "print(\"  - Dataset de petite taille\")\n",
    "print(\"  - Variables limitees\")\n",
    "print(\"  - Peut manquer des patterns complexes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nFin du projet - Arbres de Decision et Applications\")\n",
    "print(\"Tous les notebooks ont ete completes avec succes.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partie 4 - Sur-apprentissage, Forets Aleatoires et Boosting\n",
    "\n",
    "Ce notebook analyse le phenomene de sur-apprentissage et explore les methodes d'ensemble.\n",
    "\n",
    "**Contenu:**\n",
    "- Analyse du sur-apprentissage en fonction de la profondeur\n",
    "- Effet de min_samples_leaf\n",
    "- Forets aleatoires (RandomForest)\n",
    "- Boosting (AdaBoost)\n",
    "- Comparaison des methodes\n",
    "\n",
    "**Auteur**: Projet Data Mining - Arbres de Decision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importation des bibliotheques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "# Configuration\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chargement et preparation des donnees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement depuis GitHub\n",
    "base_url = 'https://raw.githubusercontent.com/NassimZahri/Data_Mining/main/data/'\n",
    "df = pd.read_csv(base_url + 'credit_simple.csv')\n",
    "\n",
    "# Preparation\n",
    "X = pd.get_dummies(df.drop('defaut', axis=1))\n",
    "y = df['defaut'].map({'oui': 1, 'non': 0})\n",
    "\n",
    "# Division train/test (70%/30%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Ensemble d'entrainement: {len(X_train)} exemples\")\n",
    "print(f\"Ensemble de test: {len(X_test)} exemples\")\n",
    "print(f\"\\nDistribution des classes (train):\")\n",
    "print(y_train.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analyse du sur-apprentissage avec max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test de differentes profondeurs\n",
    "depths = [1, 2, 3, 4, 5, 6, 7, 8, None]  # None = pas de limite\n",
    "\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "train_f1 = []\n",
    "test_f1 = []\n",
    "\n",
    "for depth in depths:\n",
    "    model = DecisionTreeClassifier(max_depth=depth, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    \n",
    "    # Metriques\n",
    "    train_accuracies.append(accuracy_score(y_train, y_pred_train))\n",
    "    test_accuracies.append(accuracy_score(y_test, y_pred_test))\n",
    "    train_f1.append(f1_score(y_train, y_pred_train, zero_division=0))\n",
    "    test_f1.append(f1_score(y_test, y_pred_test, zero_division=0))\n",
    "\n",
    "# Affichage des resultats\n",
    "depth_labels = [str(d) if d is not None else 'None' for d in depths]\n",
    "results_depth = pd.DataFrame({\n",
    "    'max_depth': depth_labels,\n",
    "    'Train Accuracy': train_accuracies,\n",
    "    'Test Accuracy': test_accuracies,\n",
    "    'Train F1': train_f1,\n",
    "    'Test F1': test_f1\n",
    "})\n",
    "\n",
    "print(\"Resultats en fonction de max_depth:\")\n",
    "results_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation du sur-apprentissage\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Accuracy\n",
    "x_pos = range(len(depths))\n",
    "axes[0].plot(x_pos, train_accuracies, 'o-', label='Train', color='#3498db', linewidth=2, markersize=8)\n",
    "axes[0].plot(x_pos, test_accuracies, 's-', label='Test', color='#e74c3c', linewidth=2, markersize=8)\n",
    "axes[0].set_xlabel('max_depth')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].set_title('Accuracy vs Profondeur de l\\'Arbre')\n",
    "axes[0].set_xticks(x_pos)\n",
    "axes[0].set_xticklabels(depth_labels)\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# F1-Score\n",
    "axes[1].plot(x_pos, train_f1, 'o-', label='Train', color='#3498db', linewidth=2, markersize=8)\n",
    "axes[1].plot(x_pos, test_f1, 's-', label='Test', color='#e74c3c', linewidth=2, markersize=8)\n",
    "axes[1].set_xlabel('max_depth')\n",
    "axes[1].set_ylabel('F1-Score')\n",
    "axes[1].set_title('F1-Score vs Profondeur de l\\'Arbre')\n",
    "axes[1].set_xticks(x_pos)\n",
    "axes[1].set_xticklabels(depth_labels)\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nObservation:\")\n",
    "print(\"- Quand la profondeur augmente, l'accuracy sur le train augmente (memorisation)\")\n",
    "print(\"- L'accuracy sur le test peut diminuer (sur-apprentissage)\")\n",
    "print(\"- L'ecart entre train et test indique le niveau de sur-apprentissage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Effet de min_samples_leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test de differentes valeurs de min_samples_leaf\n",
    "min_samples_values = [1, 2, 3, 5, 10]\n",
    "\n",
    "train_acc_samples = []\n",
    "test_acc_samples = []\n",
    "\n",
    "for min_samples in min_samples_values:\n",
    "    model = DecisionTreeClassifier(min_samples_leaf=min_samples, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    train_acc_samples.append(model.score(X_train, y_train))\n",
    "    test_acc_samples.append(model.score(X_test, y_test))\n",
    "\n",
    "# Visualisation\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(min_samples_values, train_acc_samples, 'o-', label='Train', color='#3498db', linewidth=2, markersize=8)\n",
    "plt.plot(min_samples_values, test_acc_samples, 's-', label='Test', color='#e74c3c', linewidth=2, markersize=8)\n",
    "plt.xlabel('min_samples_leaf')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Effet de min_samples_leaf sur les Performances')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"Observation:\")\n",
    "print(\"- Un min_samples_leaf plus eleve reduit le sur-apprentissage\")\n",
    "print(\"- Mais un seuil trop eleve peut causer du sous-apprentissage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Forets Aleatoires (Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test avec differents nombres d'estimateurs\n",
    "n_estimators_list = [10, 25, 50, 100, 200]\n",
    "\n",
    "rf_train_acc = []\n",
    "rf_test_acc = []\n",
    "\n",
    "for n_est in n_estimators_list:\n",
    "    rf = RandomForestClassifier(n_estimators=n_est, random_state=42, n_jobs=-1)\n",
    "    rf.fit(X_train, y_train)\n",
    "    \n",
    "    rf_train_acc.append(rf.score(X_train, y_train))\n",
    "    rf_test_acc.append(rf.score(X_test, y_test))\n",
    "\n",
    "# Resultats\n",
    "rf_results = pd.DataFrame({\n",
    "    'n_estimators': n_estimators_list,\n",
    "    'Train Accuracy': rf_train_acc,\n",
    "    'Test Accuracy': rf_test_acc\n",
    "})\n",
    "\n",
    "print(\"Random Forest - Resultats selon n_estimators:\")\n",
    "rf_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation Random Forest\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(n_estimators_list, rf_train_acc, 'o-', label='Train', color='#27ae60', linewidth=2, markersize=8)\n",
    "plt.plot(n_estimators_list, rf_test_acc, 's-', label='Test', color='#8e44ad', linewidth=2, markersize=8)\n",
    "plt.xlabel('Nombre d\\'arbres (n_estimators)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Random Forest - Performances selon le Nombre d\\'Arbres')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modele Random Forest optimal\n",
    "rf_optimal = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_optimal.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = rf_optimal.predict(X_test)\n",
    "\n",
    "print(\"Random Forest (n_estimators=100) - Rapport de classification:\")\n",
    "print(\"=\" * 60)\n",
    "print(classification_report(y_test, y_pred_rf, target_names=['Non', 'Oui']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importance des features pour Random Forest\n",
    "rf_importance = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': rf_optimal.feature_importances_\n",
    "}).sort_values('Importance', ascending=True)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(rf_importance['Feature'], rf_importance['Importance'], color='#27ae60')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Importance des Features - Random Forest')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Boosting avec AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaBoost avec differents nombres d'estimateurs\n",
    "ada_estimators_list = [10, 25, 50, 100, 200]\n",
    "\n",
    "ada_train_acc = []\n",
    "ada_test_acc = []\n",
    "\n",
    "for n_est in ada_estimators_list:\n",
    "    ada = AdaBoostClassifier(n_estimators=n_est, random_state=42)\n",
    "    ada.fit(X_train, y_train)\n",
    "    \n",
    "    ada_train_acc.append(ada.score(X_train, y_train))\n",
    "    ada_test_acc.append(ada.score(X_test, y_test))\n",
    "\n",
    "# Resultats\n",
    "ada_results = pd.DataFrame({\n",
    "    'n_estimators': ada_estimators_list,\n",
    "    'Train Accuracy': ada_train_acc,\n",
    "    'Test Accuracy': ada_test_acc\n",
    "})\n",
    "\n",
    "print(\"AdaBoost - Resultats selon n_estimators:\")\n",
    "ada_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation AdaBoost\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(ada_estimators_list, ada_train_acc, 'o-', label='Train', color='#e67e22', linewidth=2, markersize=8)\n",
    "plt.plot(ada_estimators_list, ada_test_acc, 's-', label='Test', color='#c0392b', linewidth=2, markersize=8)\n",
    "plt.xlabel('Nombre d\\'estimateurs (n_estimators)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('AdaBoost - Performances selon le Nombre d\\'Estimateurs')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modele AdaBoost optimal\n",
    "ada_optimal = AdaBoostClassifier(n_estimators=50, random_state=42)\n",
    "ada_optimal.fit(X_train, y_train)\n",
    "\n",
    "y_pred_ada = ada_optimal.predict(X_test)\n",
    "\n",
    "print(\"AdaBoost (n_estimators=50) - Rapport de classification:\")\n",
    "print(\"=\" * 60)\n",
    "print(classification_report(y_test, y_pred_ada, target_names=['Non', 'Oui']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Comparaison globale des methodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaison de toutes les methodes\n",
    "models = {\n",
    "    'Decision Tree (depth=3)': DecisionTreeClassifier(max_depth=3, random_state=42),\n",
    "    'Decision Tree (no limit)': DecisionTreeClassifier(random_state=42),\n",
    "    'Random Forest (100)': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'AdaBoost (50)': AdaBoostClassifier(n_estimators=50, random_state=42)\n",
    "}\n",
    "\n",
    "comparison_results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    train_acc = model.score(X_train, y_train)\n",
    "    test_acc = model.score(X_test, y_test)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "    \n",
    "    # Validation croisee\n",
    "    cv_scores = cross_val_score(model, X, y, cv=5)\n",
    "    \n",
    "    comparison_results.append({\n",
    "        'Modele': name,\n",
    "        'Train Accuracy': f'{train_acc:.2%}',\n",
    "        'Test Accuracy': f'{test_acc:.2%}',\n",
    "        'F1-Score': f'{f1:.2%}',\n",
    "        'CV Mean': f'{cv_scores.mean():.2%}',\n",
    "        'CV Std': f'{cv_scores.std():.2%}'\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_results)\n",
    "print(\"Comparaison des differentes methodes:\")\n",
    "print(\"=\" * 80)\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphique de comparaison\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "model_names = list(models.keys())\n",
    "x = np.arange(len(model_names))\n",
    "width = 0.35\n",
    "\n",
    "train_accs = [float(r['Train Accuracy'].strip('%'))/100 for r in comparison_results]\n",
    "test_accs = [float(r['Test Accuracy'].strip('%'))/100 for r in comparison_results]\n",
    "\n",
    "bars1 = ax.bar(x - width/2, train_accs, width, label='Train', color='#3498db')\n",
    "bars2 = ax.bar(x + width/2, test_accs, width, label='Test', color='#e74c3c')\n",
    "\n",
    "ax.set_xlabel('Modele')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Comparaison des Performances des Differentes Methodes')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(model_names, rotation=15, ha='right')\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 1.1)\n",
    "\n",
    "# Ajouter les valeurs sur les barres\n",
    "for bar in bars1:\n",
    "    height = bar.get_height()\n",
    "    ax.annotate(f'{height:.1%}', xy=(bar.get_x() + bar.get_width()/2, height),\n",
    "                xytext=(0, 3), textcoords=\"offset points\", ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "for bar in bars2:\n",
    "    height = bar.get_height()\n",
    "    ax.annotate(f'{height:.1%}', xy=(bar.get_x() + bar.get_width()/2, height),\n",
    "                xytext=(0, 3), textcoords=\"offset points\", ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Analyse et Conclusions\n",
    "\n",
    "### Sur-apprentissage:\n",
    "- Un arbre de decision sans limite de profondeur a tendance a sur-apprendre\n",
    "- L'ecart entre accuracy train et test est un indicateur de sur-apprentissage\n",
    "- Les parametres max_depth et min_samples_leaf permettent de controler ce phenomene\n",
    "\n",
    "### Forets Aleatoires:\n",
    "- Combinent plusieurs arbres pour reduire la variance\n",
    "- Plus stables que les arbres individuels\n",
    "- Le nombre d'arbres (n_estimators) affecte peu les performances apres un certain seuil\n",
    "\n",
    "### AdaBoost:\n",
    "- Methode de boosting qui combine des classifieurs faibles\n",
    "- Se concentre sur les exemples mal classes\n",
    "- Peut atteindre de bonnes performances mais risque de sur-apprentissage\n",
    "\n",
    "### Recommandations:\n",
    "1. Pour l'interpretabilite: utiliser un arbre de decision avec profondeur limitee\n",
    "2. Pour la performance: utiliser Random Forest ou AdaBoost\n",
    "3. Toujours valider avec cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Fin du notebook - Sur-apprentissage et methodes d'ensemble\")\n",
    "print(\"Le notebook suivant presentera l'application metier.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partie 3 - Comparaison avec Scikit-Learn\n",
    "\n",
    "Ce notebook compare notre implementation \"from scratch\" avec l'implementation de sklearn.\n",
    "\n",
    "**Contenu:**\n",
    "- Entrainement avec DecisionTreeClassifier de sklearn\n",
    "- Comparaison des performances\n",
    "- Visualisation de l'arbre sklearn\n",
    "- Analyse des resultats\n",
    "\n",
    "**Auteur**: Projet Data Mining - Arbres de Decision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importation des bibliotheques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree, export_text\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Configuration\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chargement des donnees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement depuis GitHub\n",
    "base_url = 'https://raw.githubusercontent.com/NassimZahri/Data_Mining/main/data/'\n",
    "df = pd.read_csv(base_url + 'credit_simple.csv')\n",
    "\n",
    "print(\"Dimensions du dataset:\", df.shape)\n",
    "print(\"\\nDistribution de la variable cible:\")\n",
    "print(df['defaut'].value_counts())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparation des donnees\n",
    "X = pd.get_dummies(df.drop('defaut', axis=1))\n",
    "y = df['defaut'].map({'oui': 1, 'non': 0})\n",
    "\n",
    "# Division train/test (70%/30%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Taille ensemble d'entrainement: {len(X_train)}\")\n",
    "print(f\"Taille ensemble de test: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Notre implementation \"From Scratch\"\n",
    "\n",
    "Re-implementation des fonctions du notebook precedent pour comparaison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classes et fonctions de notre implementation\n",
    "class Node:\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None, \n",
    "                 value=None, samples=0, impurity=0.0):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value\n",
    "        self.samples = samples\n",
    "        self.impurity = impurity\n",
    "    \n",
    "    def is_leaf(self):\n",
    "        return self.value is not None\n",
    "\n",
    "\n",
    "def gini(y):\n",
    "    if len(y) == 0:\n",
    "        return 0\n",
    "    counts = Counter(y)\n",
    "    total = len(y)\n",
    "    probs = [count / total for count in counts.values()]\n",
    "    return 1 - sum(p**2 for p in probs)\n",
    "\n",
    "\n",
    "def best_split(X, y):\n",
    "    best_gain = 0\n",
    "    best_feature = None\n",
    "    best_threshold = None\n",
    "    parent_impurity = gini(y)\n",
    "    n_samples = len(y)\n",
    "    \n",
    "    for feature in X.columns:\n",
    "        thresholds = sorted(X[feature].unique())\n",
    "        for threshold in thresholds:\n",
    "            left_mask = X[feature] <= threshold\n",
    "            right_mask = X[feature] > threshold\n",
    "            left_y = y[left_mask]\n",
    "            right_y = y[right_mask]\n",
    "            \n",
    "            if len(left_y) == 0 or len(right_y) == 0:\n",
    "                continue\n",
    "            \n",
    "            w_left = len(left_y) / n_samples\n",
    "            w_right = len(right_y) / n_samples\n",
    "            child_impurity = w_left * gini(left_y) + w_right * gini(right_y)\n",
    "            gain = parent_impurity - child_impurity\n",
    "            \n",
    "            if gain > best_gain:\n",
    "                best_gain = gain\n",
    "                best_feature = feature\n",
    "                best_threshold = threshold\n",
    "    \n",
    "    return best_feature, best_threshold, best_gain\n",
    "\n",
    "\n",
    "def build_tree(X, y, depth=0, max_depth=3, min_samples_leaf=1):\n",
    "    n_samples = len(y)\n",
    "    n_classes = len(set(y))\n",
    "    impurity = gini(y)\n",
    "    \n",
    "    if n_classes == 1 or depth >= max_depth or n_samples < min_samples_leaf * 2:\n",
    "        majority_class = y.mode()[0]\n",
    "        return Node(value=majority_class, samples=n_samples, impurity=impurity)\n",
    "    \n",
    "    feature, threshold, gain = best_split(X, y)\n",
    "    \n",
    "    if feature is None or gain <= 0:\n",
    "        majority_class = y.mode()[0]\n",
    "        return Node(value=majority_class, samples=n_samples, impurity=impurity)\n",
    "    \n",
    "    left_mask = X[feature] <= threshold\n",
    "    right_mask = X[feature] > threshold\n",
    "    \n",
    "    left_subtree = build_tree(X[left_mask], y[left_mask], depth + 1, max_depth, min_samples_leaf)\n",
    "    right_subtree = build_tree(X[right_mask], y[right_mask], depth + 1, max_depth, min_samples_leaf)\n",
    "    \n",
    "    return Node(feature=feature, threshold=threshold, left=left_subtree, \n",
    "                right=right_subtree, samples=n_samples, impurity=impurity)\n",
    "\n",
    "\n",
    "def predict_one(x, node):\n",
    "    if node.is_leaf():\n",
    "        return node.value\n",
    "    if x[node.feature] <= node.threshold:\n",
    "        return predict_one(x, node.left)\n",
    "    else:\n",
    "        return predict_one(x, node.right)\n",
    "\n",
    "\n",
    "def predict(X, tree):\n",
    "    predictions = []\n",
    "    for idx in X.index:\n",
    "        pred = predict_one(X.loc[idx], tree)\n",
    "        predictions.append(pred)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Entrainement et comparaison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notre implementation\n",
    "print(\"Entrainement de notre arbre 'from scratch'...\")\n",
    "our_tree = build_tree(X_train, y_train, max_depth=3)\n",
    "our_predictions_train = predict(X_train, our_tree)\n",
    "our_predictions_test = predict(X_test, our_tree)\n",
    "\n",
    "our_acc_train = accuracy_score(y_train, our_predictions_train)\n",
    "our_acc_test = accuracy_score(y_test, our_predictions_test)\n",
    "\n",
    "print(f\"  Accuracy (train): {our_acc_train:.2%}\")\n",
    "print(f\"  Accuracy (test): {our_acc_test:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation sklearn\n",
    "print(\"\\nEntrainement de l'arbre sklearn...\")\n",
    "sklearn_tree = DecisionTreeClassifier(max_depth=3, criterion='gini', random_state=42)\n",
    "sklearn_tree.fit(X_train, y_train)\n",
    "\n",
    "sklearn_predictions_train = sklearn_tree.predict(X_train)\n",
    "sklearn_predictions_test = sklearn_tree.predict(X_test)\n",
    "\n",
    "sklearn_acc_train = accuracy_score(y_train, sklearn_predictions_train)\n",
    "sklearn_acc_test = accuracy_score(y_test, sklearn_predictions_test)\n",
    "\n",
    "print(f\"  Accuracy (train): {sklearn_acc_train:.2%}\")\n",
    "print(f\"  Accuracy (test): {sklearn_acc_test:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Tableau comparatif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tableau de comparaison\n",
    "comparison_data = {\n",
    "    'Metrique': ['Accuracy (Train)', 'Accuracy (Test)'],\n",
    "    'Notre Implementation': [f'{our_acc_train:.2%}', f'{our_acc_test:.2%}'],\n",
    "    'Sklearn': [f'{sklearn_acc_train:.2%}', f'{sklearn_acc_test:.2%}']\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"Comparaison des performances:\")\n",
    "print(\"=\" * 60)\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Matrices de confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrices de confusion\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Notre implementation\n",
    "cm_our = confusion_matrix(y_test, our_predictions_test)\n",
    "axes[0].imshow(cm_our, cmap='Blues')\n",
    "axes[0].set_title('Notre Implementation')\n",
    "axes[0].set_xlabel('Prediction')\n",
    "axes[0].set_ylabel('Vraie valeur')\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        axes[0].text(j, i, str(cm_our[i, j]), ha='center', va='center', fontsize=20)\n",
    "axes[0].set_xticks([0, 1])\n",
    "axes[0].set_yticks([0, 1])\n",
    "axes[0].set_xticklabels(['Non', 'Oui'])\n",
    "axes[0].set_yticklabels(['Non', 'Oui'])\n",
    "\n",
    "# Sklearn\n",
    "cm_sklearn = confusion_matrix(y_test, sklearn_predictions_test)\n",
    "axes[1].imshow(cm_sklearn, cmap='Greens')\n",
    "axes[1].set_title('Sklearn')\n",
    "axes[1].set_xlabel('Prediction')\n",
    "axes[1].set_ylabel('Vraie valeur')\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        axes[1].text(j, i, str(cm_sklearn[i, j]), ha='center', va='center', fontsize=20)\n",
    "axes[1].set_xticks([0, 1])\n",
    "axes[1].set_yticks([0, 1])\n",
    "axes[1].set_xticklabels(['Non', 'Oui'])\n",
    "axes[1].set_yticklabels(['Non', 'Oui'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualisation de l'arbre sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation graphique de l'arbre sklearn\n",
    "plt.figure(figsize=(20, 10))\n",
    "plot_tree(\n",
    "    sklearn_tree, \n",
    "    feature_names=X.columns.tolist(),\n",
    "    class_names=['Non', 'Oui'],\n",
    "    filled=True,\n",
    "    rounded=True,\n",
    "    fontsize=10\n",
    ")\n",
    "plt.title('Arbre de Decision (sklearn)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage textuel de l'arbre sklearn\n",
    "print(\"Structure de l'arbre sklearn:\")\n",
    "print(\"=\" * 60)\n",
    "tree_rules = export_text(sklearn_tree, feature_names=X.columns.tolist())\n",
    "print(tree_rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Rapport de classification detaille"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Rapport de classification (sklearn sur ensemble de test):\")\n",
    "print(\"=\" * 60)\n",
    "print(classification_report(y_test, sklearn_predictions_test, target_names=['Non', 'Oui']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Importance des features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importance des features (sklearn)\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': sklearn_tree.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"Importance des features:\")\n",
    "feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphique d'importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importance['Feature'], feature_importance['Importance'], color='steelblue')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Importance des Features dans l\\'Arbre de Decision')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Conclusion\n",
    "\n",
    "### Observations:\n",
    "\n",
    "1. **Performances similaires**: Notre implementation et sklearn produisent des resultats comparables, validant notre algorithme.\n",
    "\n",
    "2. **Avantages de sklearn**:\n",
    "   - Optimisations de performance\n",
    "   - Fonctionnalites supplementaires (visualisation, importance des features)\n",
    "   - Code teste et maintenu\n",
    "\n",
    "3. **Valeur pedagogique**: L'implementation \"from scratch\" permet de comprendre en profondeur le fonctionnement des arbres de decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Fin du notebook - Comparaison avec sklearn\")\n",
    "print(\"Le notebook suivant analysera le sur-apprentissage et les forets aleatoires.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}